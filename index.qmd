---
title: "tidymodels for Production"
author: "Max Kuhn and Emil Hvitfeldt"
institute: "Posit PBC"
filters:
  - include-code-files
---

# Slides + sources on github: <br> `topepo/tidymodels-for-production`  {background-color="#B25D91FF"}

```{r}
#| label: pkgs
#| results: hide
#| echo: false
library(tidymodels)
library(butcher)
library(bundle)
theme_set(theme_bw())
```

## Production Aspects for ML

- The Data
- The Model
- Infrastructure
- Post-Deployment

## Example data

Data on [daily ridership](https://bookdown.org/max/FES/chicago-intro.html) on the Chicago "L" trains:

```{r}
#| label: chi-data
library(lubridate)

chi_data <- 
  Chicago |> 
  mutate(day = wday(date, label = TRUE)) |>
  select(ridership, date, day, Clark_Lake:Irving_Park)

chicago_original <- chi_data |> filter(date <= as.Date("2014-01-01"))
# 4,718 rows

chicago_updated  <- chi_data |> filter(date <= as.Date("2015-01-01")) 
# 5,093 rows

chicago_last     <- chi_data |> filter(date  > as.Date("2015-01-01")) 
# 605 rows
```


## Two example models

Here are two models (a single decision tree and a boosted decision tree) that we'll use: 

<br>

```{r}
#| label: chicago-trees
library(tidymodels)

# A CART classification tree
cart_wflow <- workflow(ridership ~ ., decision_tree(mode = "regression"))
cart_fit <- fit(cart_wflow, chicago_original)

# An xgboost collection of trees
xgb_wflow <- workflow(ridership ~ ., boost_tree(mode = "regression"))
xgb_fit <- fit(xgb_wflow, chicago_original)
```


## Ingesting Data

We often get our data in some markup format (e.g., cvs, json, etc.) that may not have the right context compared to the original training/testing data. 

 - How to map categories to indicators.
 - Timezone for dates (compared to the original data)

. . .

How does R (and tidymodels) deal with this? 

## Data Consistency

tidymodels uses a "zero-row slice" of the data to record important aspects of the data:

```{r}
#| label: ptype
ptype <- chicago_original[0,]
str(ptype)
```

<br>

It (and vetiver) will ensure that newly ingested data for prediction are encoded the same way and [alert the user otherwise](https://hardhat.tidymodels.org/reference/scream.html).


## Preparing and saving the model object

There are one or two operations you might want to use on your model object: 

 - [butcher](https://butcher.tidymodels.org/): remove ancillary data that are not used for prediction. 
 - [bundle](https://rstudio.github.io/bundle/): ensure that _all_ of the model data are captured for production. 

 . . .

 Both of these are automatically used by `vetiver`.
 
## Saved Model Size 

```{r}
#| label: cart-size
#| error: true
# How much space does the model take?
lobstr::obj_size(cart_fit)
```

Also, `butcher::weigh(cart_fit)` will tell you how much space each element of the model will consume.

. . .

```{r}
#| label: botcher-weigh
#| error: true
butcher::weigh(cart_fit)
```

## Trimming the model

The `butcher` package can be used to remove anything that isn't needed for prediction. 

<br>

```{r}
#| label: cart-trim

library(butcher)
# Before: 
cart_fit |> object.size() |> print(units = "Mb")

# After
cart_fit |> butcher() |> object.size() |> print(units = "Mb")
```

## Getting _Everything_ that Defines the Model

There are a few model types that don't follow traditional behavior, in that the model object doesn't contain everything the model needs. 

- Examples: xgboost, lightgbm, catboost, bart, tensorflow, and others 

. . .

<br> 

To productionize these, we need to keep _all_ of the model data in a single object. 

<br> 

That's what the bundle package does. 

## Boosting Example

Consider our previously fit xgboost model. It retains the model information in a pointer to external memory:

<br>

```{r}
#| label: chicago-boost
xgb_fit |> extract_fit_engine() |> pluck("ptr")
```

<br>

Using the usual `save()` command won't capture this extra data, so the model won't work outside of this R session. 

## Bundling

The interface is very simple: 

<br>

```{r}
#| label: bundled
library(bundle)

xgb_bund_fit <- bundle(xgb_fit)
xgb_bund_fit

xgb_fit_remade <- unbundle(xgb_bund_fit)
xgb_fit_remade |> extract_fit_engine() |> pluck("ptr")
```

Note that the pointer references a different location in memory than the original object.  

# Model Deployment {background-color="#D04E59FF"}

## Deploying with SQL

Some model types (linear models, trees) can efficiently be turned into SQL expressions. This allows us to take a fitted model and have it predict directly in a database.

Simiarly this can be done with feature engineering and postprocessing too.

The [orbital](https://orbital.tidymodels.org/) package interfaces directly with tidymodels workflows.

## Using orbital

Apply `orbital()` to a fitted workflow. 

(`tree` and `tree_depth` are set low for easy viewing on slides)

```{r}
#| label: orbital-setup
rec_spec <- recipe(ridership ~ ., data = chicago_original) |>
  step_rm(date, day) |>
  step_normalize(all_numeric_predictors())

stump_spec <- boost_tree(mode = "regression", trees = 5, tree_depth = 2)

xgb_rec_wflow <- workflow(rec_spec, stump_spec)
xgb_rec_fit <- fit(xgb_rec_wflow, chicago_original)

library(orbital)
xgb_orb <- orbital(xgb_rec_fit)
```

## Orbital objects

The orbital object only contains the sufficient calculations needed to perform predictions.

```{r}
#| label: orbital-object
#| message: true
xgb_orb
```

## Predicting with orbital objects

::: {.columns}
::: {.column}

```{r}
#| label: orbital-predict
predict(xgb_orb, chicago_original)
```

:::
::: {.column}

```{r}
#| label: workflow-predict
predict(xgb_rec_fit, chicago_original)
```

:::
:::

## Predicting in databases

If you have a connection to a database with the data in the correct format, then you can predict on it directly.

```{r}
#| label: database-predict
library(DBI)
library(RSQLite)

con <- dbConnect(SQLite(), path = ":memory:")
chicago_original_sqlite <- copy_to(con, chicago_original)

predict(xgb_orb, chicago_original_sqlite)
```

## Generating SQL

using `show_query()` or `orbital_sql()` will generate the SQL.

```{r}
#| label: show-query
show_query(xgb_orb, con)
```

## Versioning and Deploying with vetiver

[vetiver](https://vetiver.tidymodels.org/) is a tool for deploying models using the well-known [REST API](https://en.wikipedia.org/wiki/REST). 

- There are R and Python versions of the package. 

- Perhaps more than any other deployment tool, it is designed to be simple and straightforward. 

- It is also designed to correctly ingest data and make deployment and documentation very easy. 

- vetiver exploits the tools in tidymodels for tracking package dependencies. 

## Convert the model to vetiver

We start by ingesting the model and information about it:

```{r}
#| label: make-vetiver-model
library(vetiver)
xgb_vet <- vetiver_model(xgb_fit, model_name = "boosted-chicago")
xgb_vet

# Example of stored information: 
xgb_vet$metadata$required_pkgs
```


## Pinning the model

Although vetiver can be used on its own, it works very nicely with the [pins](https://pins.rstudio.com/) package. 

This can use many [cloud storage](https://pins.rstudio.com/) options and can save, share, and version the model:

<br>

```{r}
#| label: pin

library(pins)
model_board <- board_temp(versioned = TRUE)
model_board |> vetiver_pin_write(xgb_vet)
```


## Model updating

As more data is accrued, we often automate model updates, refitting the same model on the new corpus. We can add the new version to the model board: 

```{r}
#| label: boost-update
xgb_new_vet <- 
  fit(xgb_wflow, chicago_updated) |> 
  vetiver_model(model_name = "boosted-chicago")

model_board |> vetiver_pin_write(xgb_new_vet)
model_board |> pin_versions("boosted-chicago")
```

## Reverting models

Suppose the data pull for the last model was incorrect, and we need to revert to an earlier model version.

<br> 

```{r}
#| label: revert-model
last_best_version <- 
  model_board |> 
  pin_versions("boosted-chicago") |> 
  slice(1) |> 
  pluck("version")

model_board |> 
  pin_download("boosted-chicago", version = last_best_version)
```

`pin_version_delete()` could remove it too. 

## Deploying models

In R, vetiver uses the [plumber](https://www.rplumber.io/) package to deploy models: 

<br> 

```{r}
#| label: boost-deploy

library(plumber)
vetiver_api(pr(), xgb_vet)
```

## Plumber deployment file

`vetiver_write_plumber(model_board, "boosted-chicago")` will write out a template for using plumber to deploy: 

<br> 

```r 
{{< include plumber.R >}}
```

## Docker deployment file

`vetiver_write_docker(xgb_vet)` will write out a Docker template:

<br> 

```
# Generated by the vetiver package; edit with care

FROM rocker/r-ver:4.5.2
ENV RENV_CONFIG_REPOS_OVERRIDE https://packagemanager.rstudio.com/cran/latest

RUN apt-get update -qq && apt-get install -y --no-install-recommends \
  libcurl4-openssl-dev \
  libicu-dev \
  libsodium-dev \
  libssl-dev \
  libx11-dev \
  make \
  zlib1g-dev \
  && apt-get clean

COPY vetiver_renv.lock renv.lock
RUN Rscript -e "install.packages('renv')"
RUN Rscript -e "renv::restore()"
COPY plumber.R /opt/ml/plumber.R
EXPOSE 8000
ENTRYPOINT ["R", "-e", "pr <- plumber::plumb('/opt/ml/plumber.R'); pr$run(host = '0.0.0.0', port = 8000)"]
```


## Monitoring the Data and Performance

tidymodels has basic tools to compute weekly performance: 

```{r}
#| label: performance
#| eval: false
xgb_fit |> 
  augment(chicago_last) |> 
  sliding_period(
    date, period = "week",
    lookback = 0, assess_stop = 1) |> 
  mutate(
    weekly = map(splits, analysis),
    Error = map_dbl(weekly, ~ rmse(.x, ridership, .pred)$.estimate),
    Date = map_vec(weekly, ~ .x |> pluck("date") |> min())
  ) |> 
  ggplot(aes(Date, Error)) + 
  geom_point() + 
  geom_smooth()
```

## Monitoring the Data and Performance

```{r}
#| label: performance-plot
#| echo: false
xgb_fit |> 
  augment(chicago_last) |> 
  sliding_period(
    date, period = "week",
    lookback = 0, assess_stop = 1) |> 
  mutate(
    weekly = map(splits, analysis),
    Error = map_dbl(weekly, ~ rmse(.x, ridership, .pred)$.estimate),
    Date = map_vec(weekly, ~ .x |> pluck("date") |> min())
  ) |> 
  ggplot(aes(Date, Error)) + 
  geom_point() + 
  geom_smooth()
```

## Data drifts; models don't

There are a few specialized tools to make sure that your prediction population has not shifted: 

- [applicable](https://applicable.tidymodels.org/): statistical tools to measure if your prediction data are extrapolations from your training set. [(video)](https://www.youtube.com/watch?v=jQZv31s3v1Y)

- [pointblank](https://rstudio.github.io/pointblank/): data validation tools [(video)](https://www.youtube.com/watch?v=N9kaAiuAbWo)

<br>

vetiver also includes an Rmarkdown template for documenting the model and training process called "model cards."

## Other resources

 - [_MLOps with R: The Whole Game of End-to-End Data Science & Model Deployment_](https://www.youtube.com/watch?v=J32pRt1nuoY)
 - [_MLOps with vetiver in Python and R_](https://www.youtube.com/watch?v=oFQANK13-k4)
 - [_Demystifying MLOps_](https://www.youtube.com/watch?v=hzrFU5-_9-E)


# Thanks for listening! {background-color="#FAE093FF"}
